"use strict";(self.webpackChunkcore=self.webpackChunkcore||[]).push([[715],{5680:(e,n,t)=>{t.d(n,{xA:()=>p,yg:()=>m});var a=t(6540);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var s=a.createContext({}),c=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},p=function(e){var n=c(e.components);return a.createElement(s.Provider,{value:n},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},g=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=c(t),g=r,m=u["".concat(s,".").concat(g)]||u[g]||d[g]||o;return t?a.createElement(m,i(i({ref:n},p),{},{components:t})):a.createElement(m,i({ref:n},p))}));function m(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=g;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[u]="string"==typeof e?e:r,i[1]=l;for(var c=2;c<o;c++)i[c]=t[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,t)}g.displayName="MDXCreateElement"},9784:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var a=t(8168),r=(t(6540),t(5680));const o={},i="Pipeline",l={unversionedId:"UCGD-Cloud/version_1",id:"UCGD-Cloud/version_1",title:"Pipeline",description:"Introduction",source:"@site/docs/UCGD-Cloud/version_1.md",sourceDirName:"UCGD-Cloud",slug:"/UCGD-Cloud/version_1",permalink:"/Core-Docs/docs/UCGD-Cloud/version_1",draft:!1,editUrl:"https://github.com/UCGD/Core-Docs/docs/docs/UCGD-Cloud/version_1.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"The Genome in a Bottle Consortium (NIST)",permalink:"/Core-Docs/docs/Public-Datasets/NIST"},next:{title:"Introduction",permalink:"/Core-Docs/docs/UCGD-Pipeline/introduction"}},s={},c=[{value:"Introduction",id:"introduction",level:3},{value:"AWS setup and pcluster configuration",id:"aws-setup-and-pcluster-configuration",level:2},{value:"IAM User Setup for AWS ParallelCluster",id:"iam-user-setup-for-aws-parallelcluster",level:3},{value:"VPC and Subnets",id:"vpc-and-subnets",level:3},{value:"pcluster",id:"pcluster",level:3},{value:"Key configurations:",id:"key-configurations",level:3},{value:"ECR",id:"ecr",level:3},{value:"EFS",id:"efs",level:3},{value:"Nextflow",id:"nextflow",level:2}],p={toc:c},u="wrapper";function d(e){let{components:n,...t}=e;return(0,r.yg)(u,(0,a.A)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"pipeline"},"Pipeline"),(0,r.yg)("h3",{id:"introduction"},"Introduction"),(0,r.yg)("p",null,"This first version of the AWS UCGD-Cloud Pipeline is designed for variant reannotation of existing VCF data files using the Mosaic Calypso pipeline. It is an independent pipeline and is built using our existing ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/UCGD/UCGD-Pipeline"},"UCGD-Pipeline")," software base."),(0,r.yg)("p",null,"To process data in the cloud, ",(0,r.yg)("inlineCode",{parentName:"p"},"pcluster")," is used to build the AWS infrastructure. An AMI startup script provisions the necessary ",(0,r.yg)("inlineCode",{parentName:"p"},"EC2")," compute instance, and the same AMI is used to create a ",(0,r.yg)("inlineCode",{parentName:"p"},"pcluster")," image for the ",(0,r.yg)("inlineCode",{parentName:"p"},"AWS CloudFormation")," stack via ",(0,r.yg)("inlineCode",{parentName:"p"},"pcluster create-cluster"),"."),(0,r.yg)("p",null,"This configuration closely aligns with our current CHPC setup, though modifications to both Nextflow processes and configuration files are required."),(0,r.yg)("h2",{id:"aws-setup-and-pcluster-configuration"},"AWS setup and pcluster configuration"),(0,r.yg)("h3",{id:"iam-user-setup-for-aws-parallelcluster"},"IAM User Setup for AWS ParallelCluster"),(0,r.yg)("p",null,"To follow AWS best practices, create a dedicated IAM user with the appropriate permissions for using ",(0,r.yg)("inlineCode",{parentName:"p"},"AWS ParallelCluster"),":"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Review the ",(0,r.yg)("a",{parentName:"li",href:"https://docs.aws.amazon.com/parallelcluster/latest/ug/iam-roles-in-parallelcluster-v3.html"},"official AWS documentation")," for an overview of the necessary IAM permissions. Pay special attention to the ",(0,r.yg)("inlineCode",{parentName:"li"},"iam:PassRole")," permission."),(0,r.yg)("li",{parentName:"ul"},"For detailed guidance on configuring permissions specific to your compute environment and needs, see the ",(0,r.yg)("a",{parentName:"li",href:"https://docs.aws.amazon.com/parallelcluster/latest/ug/iam-roles-in-parallelcluster-v3.html#iam-roles-in-parallelcluster-v3-params-for-iam"},"ParallelCluster configuration section"),"."),(0,r.yg)("li",{parentName:"ul"},"AWS offers a good starting point to test correct ",(0,r.yg)("inlineCode",{parentName:"li"},"pcluster")," configuration ",(0,r.yg)("a",{parentName:"li",href:"https://docs.aws.amazon.com/parallelcluster/latest/ug/iam-roles-in-parallelcluster-v3.html#iam-roles-in-parallelcluster-v3-base-user-policy"},"policies"),".")),(0,r.yg)("h3",{id:"vpc-and-subnets"},"VPC and Subnets"),(0,r.yg)("p",null,"For optimal security and performance, use:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Public VPC and subnet for head node access."),(0,r.yg)("li",{parentName:"ul"},"Private VPC and subnet for compute nodes."),(0,r.yg)("li",{parentName:"ul"},"Please review detailed instructions for creating both private and public ",(0,r.yg)("a",{parentName:"li",href:"https://docs.aws.amazon.com/vpc/latest/userguide/create-vpc.html"},"VPC")," and ",(0,r.yg)("a",{parentName:"li",href:"https://docs.aws.amazon.com/vpc/latest/userguide/configure-subnets.html"},"Subnet")," needed for your compute environment.")),(0,r.yg)("h3",{id:"pcluster"},"pcluster"),(0,r.yg)("p",null,"For our implementation, a simple ",(0,r.yg)("inlineCode",{parentName:"p"},"t2.micro")," instance stores the ",(0,r.yg)("inlineCode",{parentName:"p"},"pcluster")," configuration files, initiates the ",(0,r.yg)("inlineCode",{parentName:"p"},"AWS CloudFormation")," stack, and provides easy access to the head node via ",(0,r.yg)("inlineCode",{parentName:"p"},"pcluster ssh")," command. This instance can be left running or started as needed."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},(0,r.yg)("em",{parentName:"strong"},"While idle, a HeadNode and a ComputeNode will remain running."))),(0,r.yg)("p",null,"Additionally, two versions of the ",(0,r.yg)("inlineCode",{parentName:"p"},"pcluster create-cluster")," configuration file are available and are constantly evolving:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"Region: us-west-2\nImage:\n  Os: alinux2023\nHeadNode:\n  InstanceType: m5a.xlarge\n  Networking:\n    SubnetId: subnet-<PUBLIC SUBNET>\n  Ssh:\n    KeyName: <IAM ACCESS KEYS>\n  LocalStorage:\n    RootVolume:\n      Size: 150\n  Image:\n    CustomAmi: <UCGD SPECIFIC AMI>\nScheduling:\n  Scheduler: slurm\n  SlurmQueues:\n    - Name: ucgd-standard\n      ComputeResources:\n        - Name: m5-3large\n          Instances:\n            - InstanceType: m5a.2xlarge\n          MinCount: 1\n          MaxCount: 10\n      CapacityType: ONDEMAND\n      Networking:\n        SubnetIds:\n          - subnet-<PRIVATE SUBNET>\n      Image:\n        CustomAmi: <UCGD SPECIFIC AMI>\n    - Name: ucgd-24xlarge\n      ComputeResources:\n        - Name: ucgd-xlarge\n          Instances:\n            - InstanceType: m7i.8xlarge\n          MinCount: 0\n          MaxCount: 10\n      CapacityType: ONDEMAND\n      Networking:\n        SubnetIds:\n          - subnet-<PRIVATE SUBNET>\n      Image:\n        CustomAmi: <UCGD SPECIFIC AMI>\nSharedStorage:\n  - Name: UCGDEFS\n    StorageType: Efs\n    MountDir: /common\n    EfsSettings:\n      FileSystemId: <EFS ID>\n")),(0,r.yg)("p",null,"Below is an example of an addition added to the above configuration file when requesting AWS SPOT instances."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"    - Name: ucgd-24xlarge\n      ComputeResources:\n        - Name: ucgd-xlarge\n          Instances:\n            - InstanceType: m5n.24xlarge\n          MinCount: 0\n          MaxCount: 10\n      CapacityType: SPOT\n      AllocationStrategy: price-capacity-optimized\n      Networking:\n        SubnetIds:\n          - subnet-0f780c14d0d08f90f\n      Image:\n        CustomAmi: ami-0714acd72001c3e34\n")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},(0,r.yg)("em",{parentName:"strong"},"Additionally, the UCGD specific AMI can be supplied or built using the supplied ",(0,r.yg)("inlineCode",{parentName:"em"},"ucgd-cluster-setup.sh")," script."))),(0,r.yg)("h3",{id:"key-configurations"},"Key configurations:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"* CapacityType: Supports both ONDEMAND and SPOT instances (listed above).\n* HeadNode InstanceType: m5.large (suggested).\n* ComputeNode InstanceType: should and can change based on compute type.\n* SharedStorage: EFS, providing elastic storage that scales as needed for genomic processing.\n")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},(0,r.yg)("em",{parentName:"strong"},"During testing, the ",(0,r.yg)("inlineCode",{parentName:"em"},"m5.large")," node was selected as smaller options did not allow the AMI build process enough memory to complete."))),(0,r.yg)("h3",{id:"ecr"},"ECR"),(0,r.yg)("p",null,"Most ",(0,r.yg)("inlineCode",{parentName:"p"},"nextflow")," processes use ",(0,r.yg)("inlineCode",{parentName:"p"},"Docker")," images stored in ",(0,r.yg)("inlineCode",{parentName:"p"},"AWS ECR"),". The following containers are currently in use:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"ucgd/bcftools\nucgd/calypso\nucgd/calypso-slivar\nucgd/fqf\nucgd/glnexus\nucgd/ucgd_utils\n")),(0,r.yg)("h3",{id:"efs"},"EFS"),(0,r.yg)("p",null,"After testing, an ",(0,r.yg)("a",{parentName:"p",href:"https://aws.amazon.com/efs/"},"EFS")," drive was selected for its cost, elasticity, and ability to be correctly shared across compute nodes."),(0,r.yg)("p",null,"Directories therein will contain required background data and can/will be updated as needed."),(0,r.yg)("p",null,"The Nextflow processing directory is called ",(0,r.yg)("inlineCode",{parentName:"p"},"nextflow"),". This directory will be where data is stored and processed. Each individual directory within should be deleted upon process completion to reduce EFS costs. Processing directories are named after the original Mosaic project by default, allowing ease of discovery when issues arise."),(0,r.yg)("p",null,"This is an example overview of the current directory structure used under ",(0,r.yg)("inlineCode",{parentName:"p"},"/common"),":"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"/common\n\u251c\u2500\u2500 Calypso\n\u251c\u2500\u2500 GFF3\n\u2502   \u251c\u2500\u2500 GRCh37\n\u2502   \u2514\u2500\u2500 GRCh38\n\u251c\u2500\u2500 Reference\n\u2502   \u251c\u2500\u2500 GRCh37\n\u2502   \u251c\u2500\u2500 GRCh38\n\u2502   \u2514\u2500\u2500 homo_sapiens\n\u2502       \u2514\u2500\u2500 GRCh38\n\u251c\u2500\u2500 Regions\n\u2502   \u251c\u2500\u2500 GRCh37\n\u2502   \u2514\u2500\u2500 GRCh38\n\u251c\u2500\u2500 nextflow\n\u2502   \u2514\u2500\u2500 [MOSAIC ID]\n\u251c\u2500\u2500 tools\n\u2514\u2500\u2500 vep_data\n    \u2514\u2500\u2500 homo_sapiens\n        \u251c\u2500\u2500 113_GRCh37\n        \u2514\u2500\u2500 113_GRCh38\n")),(0,r.yg)("h2",{id:"nextflow"},"Nextflow"),(0,r.yg)("p",null,"A Nextflow helper script has been created and will be available on the HeadNode. When launched, it will ask a series of questions needed to correctly process."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},(0,r.yg)("em",{parentName:"strong"},"It is recommended to run this process in a screen session."))),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"Example of running a re-annotation, helper script will change when full pipeline is implemented.\n$> ./calypso.sh\n\nEnter VCF file S3 path:\nEnter S3 path for final VCF files:\nEnter Mosaic Project ID:\nEnter Genomic Reference to use (GRCh38/GRCh37)\nRun as singleton? (true/false)\n")),(0,r.yg)("p",null,"The script will take the collected inputs and create a Nextflow command, then it will ask if you would like to run the displayed command."),(0,r.yg)("p",null,"The following steps will occur:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"A Mosaic processing directory will be created on the headnode."),(0,r.yg)("li",{parentName:"ul"},"A Nextflow processing directory will be created on the EFS drive ",(0,r.yg)("inlineCode",{parentName:"li"},"/common/nextflow/[MOSAIC ID]")," where processing output will result.")),(0,r.yg)("p",null,"Other scripts will be available on the HeadNode. Here is a brief description of the scripts that will be used and their intended functions:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"These scripts will be subject to change once the full pipeline is implemented, but are here for a brief overview.\n\n$> docker-login.sh\n# This script will be run at cluster creation and each Nextflow process. It allows correct docker login and ECR access on the compute node.\n\n$> docker-purge.sh\n# This script is used as needed to clean up any Docker builds on the HeadNode or elsewhere.\n\n$> HeadNode-setup.sh\n# This is the main script used to build the AMI image that pcluster will use, as well as all HeadNodes and compute nodes.\n")))}d.isMDXComponent=!0}}]);